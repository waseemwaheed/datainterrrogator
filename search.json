[
  {
    "objectID": "posts/Learning-Analytics-Dashboard-with-Google-Data-Studio/index.html",
    "href": "posts/Learning-Analytics-Dashboard-with-Google-Data-Studio/index.html",
    "title": "Learning Analytics - Dashboard with Google Data Studio",
    "section": "",
    "text": "COVID-19 has had a significant impact on our lives as a whole. Most industries have suffered from unexpected shift in customers demand.\nFew industries however have emerged unscathed from this pandemic, including retail, technology and health.\nEducation has been on the industries hit very hard, and the result of this impact is yet to be fully understood. This could well be the beginning of an unprecedented transformation across the sector.\nLearnPlatform is a US based company that is specialized in EdTech, their mission is\n\nExpand equitable access for all students to teaching and education technology that works best for them. Source\n\nThe company has a browser extension that collects data about usage of EdTech tools. They have recently announced a competition of Kaggle aimed at better understanding and measuring the scope and impact of the pandemic on inequities in the system. The hypothesis is that the pandemic has increased those inequities which is a sad but reasonable hypothesis.\nThe role of data analytics is to uncover the hidden realities in the data. That is not to say that the provided data set allows us to make any conclusion at this stageü§î.\nI have made an initial attempt to familiarize myself with the data by building a little and crude dashboard which can be accessed here. This dashboard gave me few questions to start exploring, so this is not the end (hopefullyü§û)."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html",
    "href": "posts/Coding-Challenge-Solution/index.html",
    "title": "A solution to a coding challenge",
    "section": "",
    "text": "I have recently been invited to a coding challenge which was required to be delivered in 7 days. The task was very simple üòé:\n\n\nThe aim of this exercise is to implement an ‚Äúalerting‚Äù service which will consume a file of currency conversion rates and produce alerts.\nFor the purpose of this coding exercise, you are allowed to choose a different programming language, provided that you provide us with a detailed instruction on how to build and run your program.\n\nThe format of the file will simulate a stream of currency conversion rates. Each line will be properly structured JSON (http://jsonlines.org/):\n{ ‚Äútimestamp‚Äù: 1554933784.023, ‚ÄúcurrencyPair‚Äù: ‚ÄúCNYAUD‚Äù, ‚Äúrate‚Äù: 0.39281 }\nThe fields in the JSON record are: - timestamp: the timestamp of the record in seconds since UNIX epoch, with fractional seconds specified - currencyPair: the sell and buy currencies which the rate relates to - rate: the conversion rate\nYou may assume that for each currency pair, currency conversion rates are streamed at a constant rate of one per second. ie. for two consecutive ‚ÄúCNYAUD‚Äù entries in in the input file, they will have timestamps that differ by one second:\n{ \"timestamp\": 1554933784.023, \"currencyPair\": \"CNYAUD\", \"rate\": 0.39281 }\n{ ‚Äútimestamp‚Äù: 1554933784.087, ‚ÄúcurrencyPair‚Äù: ‚ÄúUSDAUD‚Äù, ‚Äúrate‚Äù: 0.85641 } { ‚Äútimestamp‚Äù: 1554933785.023, ‚ÄúcurrencyPair‚Äù: ‚ÄúCNYAUD‚Äù, ‚Äúrate‚Äù: 0.39295 }\n\nThe alerting service should produce the following alert as a JSON string output to standard output:\n\nwhen the spot rate for a currency pair changes by more than 10% from the 5 minute average for that currency pair\n\nThe format of the alert produced should be:\n{ ‚Äútimestamp‚Äù: 1554933784.023, ‚ÄúcurrencyPair‚Äù: ‚ÄúCNYAUD‚Äù, ‚Äúalert‚Äù: ‚ÄúspotChange‚Äù }\n\nAs mentioned earlier, the task is very simple but I wanted to take the opportunity to improve the following aspects:\n\nCode readability\nUnit testing\nDeployability\n\n\n\nI use VSCODE for many tasks, and it is my main text editor so naturally, I looked for tools that play well with it. For automatic code styling, I used black which is a great code formatter. Additionally, I revised the PEP 8 guide to refresh my memory of best practices.\n\n\n\nThe idea behind unit testing is that you have to arrange your code into non-coupled components to allow for testing. In the Python ecosystem there are few options such as the unittest which comes as part of the Python Standard Library and pytest. I ended up using pytest because I wanted to learn it.\n\n\n\nPython, similar to other interpreted languages, requires a compatible version of the interpreter and the same version of packages (excluding Javascript, where every computer nowadays comes with one). This is a common issue that has many solutions. Among those solutions are the virtual\nnments such as (venv, virtualenv, conda env) and containers such as the well-known docker.\nThe simplicity and ubiquity of docker made it a simple choice üëç for me in this challenge. Once the code is written and tested, docker image description file is all that is needed. The alternative path of virtual environment was also a viable one, I just had to write environment creation scripts, one for windows and one for Unix/Linux ü•±."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html#code-readability",
    "href": "posts/Coding-Challenge-Solution/index.html#code-readability",
    "title": "A solution to a coding challenge",
    "section": "",
    "text": "I use VSCODE for many tasks, and it is my main text editor so naturally, I looked for tools that play well with it. For automatic code styling, I used black which is a great code formatter. Additionally, I revised the PEP 8 guide to refresh my memory of best practices."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html#unit-testing",
    "href": "posts/Coding-Challenge-Solution/index.html#unit-testing",
    "title": "A solution to a coding challenge",
    "section": "",
    "text": "The idea behind unit testing is that you have to arrange your code into non-coupled components to allow for testing. In the Python ecosystem there are few options such as the unittest which comes as part of the Python Standard Library and pytest. I ended up using pytest because I wanted to learn it."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html#deployability",
    "href": "posts/Coding-Challenge-Solution/index.html#deployability",
    "title": "A solution to a coding challenge",
    "section": "",
    "text": "Python, similar to other interpreted languages, requires a compatible version of the interpreter and the same version of packages (excluding Javascript, where every computer nowadays comes with one). This is a common issue that has many solutions. Among those solutions are the virtual\nnments such as (venv, virtualenv, conda env) and containers such as the well-known docker.\nThe simplicity and ubiquity of docker made it a simple choice üëç for me in this challenge. Once the code is written and tested, docker image description file is all that is needed. The alternative path of virtual environment was also a viable one, I just had to write environment creation scripts, one for windows and one for Unix/Linux ü•±."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html#assumptions",
    "href": "posts/Coding-Challenge-Solution/index.html#assumptions",
    "title": "A solution to a coding challenge",
    "section": "Assumptions",
    "text": "Assumptions\n\nOne input file can be consumed at a time.\nThe frequency at which updates arrive is fixed (1s). Hence the average, in the general case, is taken over 300 samples.\nA single stream (input file) can contain more than one currency pair."
  },
  {
    "objectID": "posts/Coding-Challenge-Solution/index.html#decisions",
    "href": "posts/Coding-Challenge-Solution/index.html#decisions",
    "title": "A solution to a coding challenge",
    "section": "Decisions",
    "text": "Decisions\n\nTo keep track of the exchange rates for each pair, I made the currency pairs keys of a dictionary which maps to sliding window deque data structure.\n\nThe rationale behind choosing a dictionary is because, each new line can be new data point for a currency pair, which means the currency pairs data need to be accessed in random order and a dictionary is the best options here where an access operation is \\(O(1)\\)\nThe rationale behind choosing a deque is that it allows for the easy creation of a sliding window. A deque has \\(O(1)\\) complexity when we append to or access the ends of the queue, which is what we are doing here.\n\nThe CurrencyPairData class is a subclass of the Observable class. This allows me to easily add callbacks to CurrencyPairData instances."
  },
  {
    "objectID": "posts/ID-Mapping-using-Selenium/index.html",
    "href": "posts/ID-Mapping-using-Selenium/index.html",
    "title": "ID Mapping using Selenium",
    "section": "",
    "text": "Context\nAt work, we have to keep track of the progress of students where each student is identified by their ID (matching by the name is a tricky thing and not reliable).\n\n\nThe problem\nInstead of a single ID, students have two!\nYou may say, that should be fine, you must have a database mapping those IDs to each other isn‚Äôt it? and the answer is yes but, there is a BUT, I can access one student at a time through an online portal, which is kinda fun, you know üôÑ.\n\n\nThe solution\nBut, then you may say, hang on, the DBA should be able to provide you with the data you need, isn‚Äôt it. To you I say, I wish that was the case, anyways.\nTo me this is the sort of thing that I like to spend my free time on üòé, so I spent an hour or so, figuring how to do it üê±‚Äçüíª. I knew that there is a nice web browser automation tool called Selenium, but I had never used at that point.\nWith a bit of Pandas, I was able to get job done and I am very happy with the result, the script is accessible here."
  },
  {
    "objectID": "posts/D3js-Explorations-2/index.html",
    "href": "posts/D3js-Explorations-2/index.html",
    "title": "D3 - Visualizing bike share stations on map",
    "section": "",
    "text": "I started today with a goal to plot data from Melbourne city particularly Melbourne Bike Share stations on google maps with marker size and color representing the size of the station and the availability of bikes respectively in addition to tooltips containing the details of each station on hover.\nMapping the data went smoothly but tooltips turned to be tricky, it took me some time to decide to postpone it to another stage. I think the problem has to do with google maps handling events not D3 so in other words events are not being passed to D3. Handling events with google maps is straight forward but finding the current location of the markers to compare with the current mouse position is not clear at this point.\nAnyway, I think the visualization is still interesting."
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "",
    "text": "Performing data analysis, in many cases, requires loading the data from a database. Database engines are optimized for the efficient handling of data storage and retrieval. If the data is structured, which means the data follows a schema, querying the database is always performed using SQL, a domain specific language for data querying among other things.\nI have been using Jupyter notebooks for 4 years now, and I think the notebook concept is transformational. Being able to mix the analysis with the compute in one linear document is great. The other day I was looking at some SQL queries and thought to myself, I wish that could write those queries in Jupyter notebooks just like I do with Python, I had previously tried Matlab and Julia, why not SQL?\nAs you would imagine, the community has thought about this question way before I did and someone has made that possible in the form of a IPython extension üòç.\nThis post is my attempt at using SQL in the Jupyter notebook environment. The data we are going to use in this excercise the 20 years of Olympic history: athletes and results from Kaggle. The analysis presented here is for practice only and for more detailed analysis of the olympic games, check Olympians are probably older ‚Äî and younger ‚Äî than you think."
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#introduction",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#introduction",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "",
    "text": "Performing data analysis, in many cases, requires loading the data from a database. Database engines are optimized for the efficient handling of data storage and retrieval. If the data is structured, which means the data follows a schema, querying the database is always performed using SQL, a domain specific language for data querying among other things.\nI have been using Jupyter notebooks for 4 years now, and I think the notebook concept is transformational. Being able to mix the analysis with the compute in one linear document is great. The other day I was looking at some SQL queries and thought to myself, I wish that could write those queries in Jupyter notebooks just like I do with Python, I had previously tried Matlab and Julia, why not SQL?\nAs you would imagine, the community has thought about this question way before I did and someone has made that possible in the form of a IPython extension üòç.\nThis post is my attempt at using SQL in the Jupyter notebook environment. The data we are going to use in this excercise the 20 years of Olympic history: athletes and results from Kaggle. The analysis presented here is for practice only and for more detailed analysis of the olympic games, check Olympians are probably older ‚Äî and younger ‚Äî than you think."
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#motivation",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#motivation",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "Motivation",
    "text": "Motivation\nI have few reasons for writing this article: 1. As a reminder for my future self, cause I tend to move on and forget üòÖ as you can imagine, 2. I hope it serves as an inspiration to you, the reader, to utilize this capability if you like it, 3. Practice SQL, which I can‚Äôt have enough of, 4. Being able to send the calculations to the data rather than bring the data to compute is a super power in the Big Data era, 5. Bringing the result of SQL queries back to Python rather than querying the database directly or using something like SQL Server Management Studio or MySQL Workbench allows us to visualize the data in whatever way we like.\nIt is important to mention that whatever we are doing here can be done within Pandas directly, however from a scalability and effeciency perspectives, the ability to do most of the filtering and summerization at the database level is a clear advantage.\nThe database can be hosted anywhere. Which means the machine on which the analysis is carried out doesn‚Äôt need to be highly resourced.\n\nWhy not an ORM?\nORM stands for Object Relational Mapping which according to wikipedia: &gt; a programming technique for converting data between incompatible type systems using object-oriented programming languages. This creates, in effect, a ‚Äúvirtual object database‚Äù that can be used from within the programming language.\nORMs hide the typical SQL interaction from the software developer, exposing the database as classes and objects with getters and setters. An typical example of an ORM are SQLAlchemy and SQLModel which builds on top SQLAlchemy. If you think that this is a good idea, I tend to agree as do most web frameworks but, the goal of this article is not to run away from SQL but to embrace it as universal tool that can be utilized on its own and in combination with most programming languages.\n\n\nWhy not Spark (or the likes)\nThis is a great question. If you have heard of Big Data tools such as Spark, this question would defintely come to your mind. Spark does what we are trying to achieve, and I am planning to cover it in a follow up writeup.\nHopefully, I managed to convince you of the benefits of being able to query databases using SQL rather than doing the same analysis in Pandas."
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#required-packages",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#required-packages",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "Required packages",
    "text": "Required packages\nI am going to use the following packages in this article:\n\nipython-sql\nPandas\nMatplotlib\nSQLAlchemy"
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#connecting-to-database",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#connecting-to-database",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "Connecting to database",
    "text": "Connecting to database\nThis first step is to establish a connection to the database you would like to work on. We are going to use the ipython-sql extension to write and pass our SQL queries. ipython-sql expects database string similar to those used by SQLAlcehmy. For more details about the database strings check this page.\nIn this article, I am going to use a local installation of Microsoft SQL Server 2019 Express. I have already downloaded MS-SQL server already and installed it. I then created a new database named OlympicsHistory and imported the two csv files from the Olympic History dataset into this database as a tables named athlete_events, noc_region .\n\n%load_ext sql\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n%sql mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?trusted_connection=yes&driver=SQL+Server"
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#how-to-handle-queries-and-results",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#how-to-handle-queries-and-results",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "How to handle queries and results",
    "text": "How to handle queries and results\nNow that you succeeded connecting to the database, let‚Äôs see how can we bring data from the database into Python. The ipython-sql extension offers three mode of access:\n1. Print the results\n2. Assignning the result of a single-line query \n3. Assignning the result of a multi-line query\nNote: ignore the details of the queries for now and let‚Äôs focus on the handling of queries, we will return to the details of the queries in the following section.\n\n1. Print the results\nYou use the magic command (to learn more) %sql for a single line and %%sql for multi-line queries.\n1.1. Single line query\n\n%sql SELECT DISTINCT TOP 3 Sport FROM   athlete_events \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nSport\n\n\nBasketball\n\n\nJudo\n\n\nFootball\n\n\n\n\n\n1.2. Multi-line query\n\n%%sql\nSELECT DISTINCT TOP 3 Sport\nFROM   athlete_events \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nSport\n\n\nBasketball\n\n\nJudo\n\n\nFootball\n\n\n\n\n\n\n\n2. Assignning the result of a single-line query\n\nresult = %sql SELECT DISTINCT TOP 3 Sport FROM   athlete_events\nprint(result)\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n+------------+\n|   Sport    |\n+------------+\n| Basketball |\n|    Judo    |\n|  Football  |\n+------------+\n\n\n\n%%sql result &lt;&lt;\nSELECT TOP 3 year,\n             Count(CASE\n                     WHEN Medal = 'Bronze' THEN 1\n                   END) Bronze,\n             Count(CASE\n                     WHEN Medal = 'Silver' THEN 1\n                   END) Silver,\n             Count(CASE\n                     WHEN Medal = 'Gold' THEN 1\n                   END) Gold\nFROM   athlete_events\nGROUP  BY Year\nORDER  BY Year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable result\n\n\n\nprint(result)\n\n+------+--------+--------+------+\n| year | Bronze | Silver | Gold |\n+------+--------+--------+------+\n| 1896 |   38   |   43   |  62  |\n| 1900 |  175   |  228   | 201  |\n| 1904 |  150   |  163   | 173  |\n+------+--------+--------+------+\n\n\nIf have used Python for data analysis, you might say this cool but, it would even nice if we could capture the result of the query as Pandas DataFrame! If you had this question in mind, to you I say, it is straight forward to that, have a look:\n\ndf = result.DataFrame()\nprint(df)\n\n   year  Bronze  Silver  Gold\n0  1896      38      43    62\n1  1900     175     228   201\n2  1904     150     163   173\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nyear\nBronze\nSilver\nGold\n\n\n\n\ncount\n3.0\n3.000000\n3.000000\n3.000000\n\n\nmean\n1900.0\n121.000000\n144.666667\n145.333333\n\n\nstd\n4.0\n72.958893\n93.852722\n73.514171\n\n\nmin\n1896.0\n38.000000\n43.000000\n62.000000\n\n\n25%\n1898.0\n94.000000\n103.000000\n117.500000\n\n\n50%\n1900.0\n150.000000\n163.000000\n173.000000\n\n\n75%\n1902.0\n162.500000\n195.500000\n187.000000\n\n\nmax\n1904.0\n175.000000\n228.000000\n201.000000\n\n\n\n\n\n\n\nI hope that you can see the power in what I just demonstrated."
  },
  {
    "objectID": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#data-analysis-using-sql",
    "href": "posts/Efficient-Data-Analysis-SQL-and-Python/Efficient-Data-Analysis-SQL-and-Python.html#data-analysis-using-sql",
    "title": "Efficient Data Analysis - SQL and Python",
    "section": "Data Analysis using SQL",
    "text": "Data Analysis using SQL\nEnough of the preparation, let‚Äôs into the fun part, analysing the data, asking and answering questions about it.\nAs a first step, let‚Äôs familiarize ourselves with the two tables we have\n\n%sql SELECT TOP 5 * FROM   athlete_events \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nID\nName\nSex\nAge\nHeight\nWeight\nTeam\nNOC\nGames\nYear\nSeason\nCity\nSport\nEvent\nMedal\n\n\n1\nA Dijiang\nM\n24\n180\n80\nChina\nCHN\n1992 Summer\n1992\nSummer\nBarcelona\nBasketball\nBasketball Men's Basketball\nNA\n\n\n2\nA Lamusi\nM\n23\n170\n60\nChina\nCHN\n2012 Summer\n2012\nSummer\nLondon\nJudo\nJudo Men's Extra-Lightweight\nNA\n\n\n3\nGunnar Nielsen Aaby\nM\n24\nNA\nNA\nDenmark\nDEN\n1920 Summer\n1920\nSummer\nAntwerpen\nFootball\nFootball Men's Football\nNA\n\n\n4\nEdgar Lindenau Aabye\nM\n34\nNA\nNA\nDenmark/Sweden\nDEN\n1900 Summer\n1900\nSummer\nParis\nTug-Of-War\nTug-Of-War Men's Tug-Of-War\nGold\n\n\n5\nChristine Jacoba Aaftink\nF\n21\n185\n82\nNetherlands\nNED\n1988 Winter\n1988\nWinter\nCalgary\nSpeed Skating\nSpeed Skating Women's 500 metres\nNA\n\n\n\n\n\n\n%sql SELECT TOP 5 * FROM   noc_regions \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nNOC\nregion\nnotes\n\n\nAFG\nAfghanistan\nNone\n\n\nAHO\nCuracao\nNetherlands Antilles\n\n\nALB\nAlbania\nNone\n\n\nALG\nAlgeria\nNone\n\n\nAND\nAndorra\nNone\n\n\n\n\n\n\n%%sql \nSELECT DISTINCT athlete_events.noc\nFROM   athlete_events\n       LEFT JOIN noc_regions\n              ON athlete_events.noc = noc_regions.noc\nWHERE  region IS NULL \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nnoc\n\n\nSGP\n\n\n\n\n\nLooking up SGP reveals that this is the code for Singapore. This begs the question, why was this code missing from the noc_regions table? Let‚Äôs further investigate this table\n\n%%sql\nSELECT *\nFROM   noc_regions\nWHERE  region LIKE '%pore' \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nNOC\nregion\nnotes\n\n\nSIN\nSingapore\nNone\n\n\n\n\n\nLet‚Äôs further investigate the athlete_events table for instances of the codes SIN and SGP\n\n%%sql\nSELECT DISTINCT team,\n                noc\nFROM   athlete_events\nWHERE  team LIKE '%pore'\n        OR noc = 'SGP'\n        OR noc = 'SIN' \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nteam\nnoc\n\n\nJune Climene\nSGP\n\n\nRika II\nSGP\n\n\nSingapore\nSGP\n\n\nSingapore-1\nSGP\n\n\nSingapore-2\nSGP\n\n\n\n\n\nOk, so the code SIN has never been used in the athlete_events. A quick web search revealed that SGP has replaced SIN in 2016.\nIn this case, I think, it would be a good idea to keep both codes in the noc_regions table. Let‚Äôs add SGP\n\n#collapse_output\n%%sql \nIF NOT EXISTS (SELECT *\n               FROM   noc_regions\n               WHERE  noc = 'SGP')\n  INSERT INTO noc_regions\n  VALUES      ('SGP',\n               'Singapore',\n               'Added by Waseem') \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\n1 rows affected.\n\n\nResourceClosedError: This result object does not return rows. It has been closed automatically.\n\n\nThere seems to be a bug in SQLAlchemy which makes ealier SQL code error. We can safely ignore this error as the code seems to be doing what is supposed to do, let‚Äôs verify:\n\n%%sql\nSELECT *\nFROM   noc_regions\nWHERE  noc = 'SGP' \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nNOC\nregion\nnotes\n\n\nSGP\nSingapore\nAdded by Waseem\n\n\n\n\n\nWe can now perform joins without any NULLs.\nLet‚Äôs begin with the data interrogation:\n\nWhich cities host the Olympics more that once?\n\n%%sql cities &lt;&lt; \nSELECT City,\n       Count(Year) AS NumTimes\nFROM   (SELECT DISTINCT year,\n                        city\n        FROM   athlete_events) city\nGROUP  BY city\nHAVING Count(year) &gt; 1\nORDER  BY numtimes DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable cities\n\n\n\ncities_df = cities.DataFrame()\ncities_df.plot.bar(x='City', y='NumTimes')\n\n&lt;AxesSubplot:xlabel='City'&gt;\n\n\n\n\n\n\n#collapse_output\n%%sql \nCREATE OR ALTER VIEW city_year AS\nSELECT DISTINCT City,\n                Year\nFROM            athlete_events\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\nResourceClosedError: This result object does not return rows. It has been closed automatically.\n\n\n\n%%sql\nSELECT City,\n       Count(Year) AS NumTimes\nFROM   city_year\nGROUP  BY City\nHAVING Count(Year) &gt; 1\nORDER  BY numtimes DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nCity\nNumTimes\n\n\nLondon\n3\n\n\nAthina\n3\n\n\nSankt Moritz\n2\n\n\nInnsbruck\n2\n\n\nLake Placid\n2\n\n\nStockholm\n2\n\n\nLos Angeles\n2\n\n\nParis\n2\n\n\n\n\n\n\n\nFind the seasons for each year as two columns (Summer, Winter)\n\n%%sql\nSELECT *\nFROM   (SELECT DISTINCT PARSENAME(REPLACE(Games, ' ', '.'), 2) AS Year,\n                        PARSENAME(REPLACE(Games, ' ', '.'), 1) AS Season\n        FROM   athlete_events) t1\n       PIVOT ( Count(Season)\n             FOR Season IN (Summer,\n                            Winter) ) piv\nORDER  BY Year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nYear\nSummer\nWinter\n\n\n1896\n1\n0\n\n\n1900\n1\n0\n\n\n1904\n1\n0\n\n\n1906\n1\n0\n\n\n1908\n1\n0\n\n\n1912\n1\n0\n\n\n1920\n1\n0\n\n\n1924\n1\n1\n\n\n1928\n1\n1\n\n\n1932\n1\n1\n\n\n1936\n1\n1\n\n\n1948\n1\n1\n\n\n1952\n1\n1\n\n\n1956\n1\n1\n\n\n1960\n1\n1\n\n\n1964\n1\n1\n\n\n1968\n1\n1\n\n\n1972\n1\n1\n\n\n1976\n1\n1\n\n\n1980\n1\n1\n\n\n1984\n1\n1\n\n\n1988\n1\n1\n\n\n1992\n1\n1\n\n\n1994\n0\n1\n\n\n1996\n1\n0\n\n\n1998\n0\n1\n\n\n2000\n1\n0\n\n\n2002\n0\n1\n\n\n2004\n1\n0\n\n\n2006\n0\n1\n\n\n2008\n1\n0\n\n\n2010\n0\n1\n\n\n2012\n1\n0\n\n\n2014\n0\n1\n\n\n2016\n1\n0\n\n\n\n\n\n\n\nWhat is the average age of participants?\n\n%%sql\nSELECT Avg(Cast(Age AS INT)) as AvgAge\nFROM   athlete_events\nWHERE  age IS NOT NULL\n       AND age &lt;&gt; 'NA' \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nAvgAge\n\n\n25\n\n\n\n\n\n\n\nWhat is the average age per season?\n\n%%sql\nSELECT Season,\n       Avg(CAST(Age AS INT)) AvgAge\nFROM   athlete_events\nWHERE  Age &lt;&gt; 'NA'\nGROUP  BY Season \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nSeason\nAvgAge\n\n\nSummer\n25\n\n\nWinter\n25\n\n\n\n\n\n\n\nIs the average age fixed across the seasons and years?\n\n%%sql result &lt;&lt;\nSELECT Year, \n       Season, \n       AVG(cast(Age as int)) AverageAge \nFROM   athlete_events\nWHERE Age is not NULL \n      AND Age &lt;&gt; 'NA'\nGROUP BY Year, \n         Season\nORDER BY Year, \n         Season\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable result\n\n\n\nresult_df = result.DataFrame()\nsummer_df = result_df[result_df['Season'] == 'Summer']\nwinter_df = result_df[result_df['Season'] == 'Winter']\n\n\nsummer_df.plot.scatter(x='Year',y='AverageAge', title='Summer Participants Average Age')\nwinter_df.plot.scatter(x='Year',y='AverageAge', title='Winter Participants Average Age')\n\n&lt;AxesSubplot:title={'center':'Winter Participants Average Age'}, xlabel='Year', ylabel='AverageAge'&gt;\n\n\n\n\n\n\n\n\nThe average for summer Olympics in 1932 looks too high, is this a valid result? if yes why did it happen?\n\n%%sql\nselect AVG(cast(Age as int)) as AvgAge from athlete_events\nleft join noc_regions on athlete_events.NOC = noc_regions.NOC\nwhere Year=1932 and Age &lt;&gt; 'NA' and Season='Summer'\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nAvgAge\n\n\n33\n\n\n\n\n\nSo, the average age was high, why is that the case? Let‚Äôs look at a more statistics about the games broken down by year and season.\n\n%%sql\nSELECT year,\n       Min(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerMinAge,\n       Max(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerMaxAge,\n       Avg(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerAvgAge,\n       Var(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerVarAge,\n       Min(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterMinAge,\n       Max(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterMaxAge,\n       Avg(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterAvgAge,\n       Var(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterVarAge\nFROM   athlete_events\nWHERE  age IS NOT NULL\n       AND age &lt;&gt; 'NA'\nGROUP  BY year\nORDER  BY year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nyear\nSummerMinAge\nSummerMaxAge\nSummerAvgAge\nSummerVarAge\nWinterMinAge\nWinterMaxAge\nWinterAvgAge\nWinterVarAge\n\n\n1896\n10\n40\n23\n22.02240143369178\nNone\nNone\nNone\nNone\n\n\n1900\n13\n71\n29\n87.57875351516955\nNone\nNone\nNone\nNone\n\n\n1904\n14\n71\n26\n76.6066534940619\nNone\nNone\nNone\nNone\n\n\n1906\n13\n54\n27\n62.617258530706444\nNone\nNone\nNone\nNone\n\n\n1908\n14\n61\n26\n61.15578403594632\nNone\nNone\nNone\nNone\n\n\n1912\n13\n67\n27\n64.84038295212197\nNone\nNone\nNone\nNone\n\n\n1920\n13\n72\n29\n68.44374620002326\nNone\nNone\nNone\nNone\n\n\n1924\n13\n81\n28\n74.08304035936791\n11\n58\n27\n48.97374171326985\n\n\n1928\n11\n97\n29\n118.93684427853675\n15\n54\n26\n37.10742966900141\n\n\n1932\n13\n96\n33\n202.85578680390805\n11\n52\n25\n33.449755356216194\n\n\n1936\n12\n74\n27\n76.48352848554916\n11\n46\n25\n24.044063840363208\n\n\n1948\n12\n84\n29\n93.24748654592041\n15\n53\n26\n32.50355943000253\n\n\n1952\n13\n65\n26\n43.83151104313369\n12\n47\n25\n26.63744994317875\n\n\n1956\n13\n67\n26\n43.08506131207035\n12\n48\n25\n25.929199837782704\n\n\n1960\n12\n65\n25\n38.08302692628261\n11\n39\n24\n20.080557267666588\n\n\n1964\n12\n60\n25\n31.975105597154663\n13\n53\n24\n21.768654166104547\n\n\n1968\n11\n68\n24\n36.317311148018874\n11\n51\n24\n18.71876641678888\n\n\n1972\n12\n69\n24\n35.930160546839694\n13\n42\n24\n19.98596052286161\n\n\n1976\n12\n70\n23\n32.802134361215664\n12\n46\n23\n21.7698320494643\n\n\n1980\n13\n70\n23\n27.768520445745068\n13\n49\n23\n18.154642359559396\n\n\n1984\n12\n60\n24\n30.05881562619138\n15\n53\n23\n16.397695807730326\n\n\n1988\n13\n70\n24\n29.150256275905534\n11\n52\n23\n17.46205581946204\n\n\n1992\n11\n62\n24\n29.238076996153065\n13\n46\n24\n17.08599797046234\n\n\n1994\nNone\nNone\nNone\nNone\n13\n46\n24\n17.603527791814635\n\n\n1996\n12\n63\n24\n30.263060247773502\nNone\nNone\nNone\nNone\n\n\n1998\nNone\nNone\nNone\nNone\n14\n50\n25\n19.702398658160618\n\n\n2000\n13\n63\n25\n29.60243153446837\nNone\nNone\nNone\nNone\n\n\n2002\nNone\nNone\nNone\nNone\n15\n48\n25\n22.23690379230238\n\n\n2004\n13\n57\n25\n31.13741181652509\nNone\nNone\nNone\nNone\n\n\n2006\nNone\nNone\nNone\nNone\n14\n52\n25\n23.876212746402683\n\n\n2008\n12\n67\n25\n32.32947785953114\nNone\nNone\nNone\nNone\n\n\n2010\nNone\nNone\nNone\nNone\n15\n51\n26\n25.120888173261157\n\n\n2012\n13\n71\n25\n32.28653359213402\nNone\nNone\nNone\nNone\n\n\n2014\nNone\nNone\nNone\nNone\n15\n55\n25\n23.517221690522096\n\n\n2016\n13\n62\n26\n30.917678201428895\nNone\nNone\nNone\nNone\n\n\n\n\n\nIt is clear that there‚Äôs a high variability in participants ages in 1932. Let‚Äôs have a closer look at the data to find out what could the reason behind that be.\nIn which sport did older participants take part in?\n\n%%sql\nselect top 10 * from athlete_events\nwhere year = '1932' and season = 'Summer' and age &lt;&gt; 'NA' and age = 96\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nID\nName\nSex\nAge\nHeight\nWeight\nTeam\nNOC\nGames\nYear\nSeason\nCity\nSport\nEvent\nMedal\n\n\n49663\nWinslow Homer\nM\n96\nNA\nNA\nUnited States\nUSA\n1932 Summer\n1932\nSummer\nLos Angeles\nArt Competitions\nArt Competitions Mixed Painting, Unknown Event\nNA\n\n\n\n\n\nOh, the older folks participated in Art Competitions. Let‚Äôs see what impact does removing Art Competitions have on the age distribution.\n\n%%sql\nSELECT year,\n       Min(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerMinAge,\n       Max(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerMaxAge,\n       Avg(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerAvgAge,\n       Var(CASE\n             WHEN season = 'Summer' THEN Cast(age AS INT)\n           END) SummerVarAge,\n       Min(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterMinAge,\n       Max(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterMaxAge,\n       Avg(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterAvgAge,\n       Var(CASE\n             WHEN season = 'Winter' THEN Cast(age AS INT)\n           END) WinterVarAge\nFROM   athlete_events\nWHERE  age IS NOT NULL\n       AND age &lt;&gt; 'NA'\n       AND Sport &lt;&gt; 'Art Competitions'\nGROUP  BY year\nORDER  BY year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nyear\nSummerMinAge\nSummerMaxAge\nSummerAvgAge\nSummerVarAge\nWinterMinAge\nWinterMaxAge\nWinterAvgAge\nWinterVarAge\n\n\n1896\n10\n40\n23\n22.02240143369178\nNone\nNone\nNone\nNone\n\n\n1900\n13\n71\n29\n87.57875351516955\nNone\nNone\nNone\nNone\n\n\n1904\n14\n71\n26\n76.6066534940619\nNone\nNone\nNone\nNone\n\n\n1906\n13\n54\n27\n62.617258530706444\nNone\nNone\nNone\nNone\n\n\n1908\n14\n61\n26\n61.15578403594632\nNone\nNone\nNone\nNone\n\n\n1912\n13\n64\n27\n62.604400100967034\nNone\nNone\nNone\nNone\n\n\n1920\n13\n72\n29\n67.98906348229981\nNone\nNone\nNone\nNone\n\n\n1924\n13\n60\n27\n54.157015903460966\n11\n58\n27\n48.97374171326985\n\n\n1928\n11\n63\n25\n47.05709575019104\n15\n54\n26\n37.10742966900141\n\n\n1932\n13\n54\n25\n38.055043191309956\n11\n52\n25\n33.449755356216194\n\n\n1936\n12\n72\n25\n35.960182685284465\n11\n46\n25\n24.044063840363208\n\n\n1948\n12\n61\n27\n50.99427202894765\n15\n53\n26\n32.50355943000253\n\n\n1952\n13\n65\n26\n43.83151104313369\n12\n47\n25\n26.63744994317875\n\n\n1956\n13\n67\n26\n43.08506131207035\n12\n48\n25\n25.929199837782704\n\n\n1960\n12\n65\n25\n38.08302692628261\n11\n39\n24\n20.080557267666588\n\n\n1964\n12\n60\n25\n31.975105597154663\n13\n53\n24\n21.768654166104547\n\n\n1968\n11\n68\n24\n36.317311148018874\n11\n51\n24\n18.71876641678888\n\n\n1972\n12\n69\n24\n35.930160546839694\n13\n42\n24\n19.98596052286161\n\n\n1976\n12\n70\n23\n32.802134361215664\n12\n46\n23\n21.7698320494643\n\n\n1980\n13\n70\n23\n27.768520445745068\n13\n49\n23\n18.154642359559396\n\n\n1984\n12\n60\n24\n30.05881562619138\n15\n53\n23\n16.397695807730326\n\n\n1988\n13\n70\n24\n29.150256275905534\n11\n52\n23\n17.46205581946204\n\n\n1992\n11\n62\n24\n29.238076996153065\n13\n46\n24\n17.08599797046234\n\n\n1994\nNone\nNone\nNone\nNone\n13\n46\n24\n17.603527791814635\n\n\n1996\n12\n63\n24\n30.263060247773502\nNone\nNone\nNone\nNone\n\n\n1998\nNone\nNone\nNone\nNone\n14\n50\n25\n19.702398658160618\n\n\n2000\n13\n63\n25\n29.60243153446837\nNone\nNone\nNone\nNone\n\n\n2002\nNone\nNone\nNone\nNone\n15\n48\n25\n22.23690379230238\n\n\n2004\n13\n57\n25\n31.13741181652509\nNone\nNone\nNone\nNone\n\n\n2006\nNone\nNone\nNone\nNone\n14\n52\n25\n23.876212746402683\n\n\n2008\n12\n67\n25\n32.32947785953114\nNone\nNone\nNone\nNone\n\n\n2010\nNone\nNone\nNone\nNone\n15\n51\n26\n25.120888173261157\n\n\n2012\n13\n71\n25\n32.28653359213402\nNone\nNone\nNone\nNone\n\n\n2014\nNone\nNone\nNone\nNone\n15\n55\n25\n23.517221690522096\n\n\n2016\n13\n62\n26\n30.917678201428895\nNone\nNone\nNone\nNone\n\n\n\n\n\nI am statisfied with this now, it seems to match the result reported in here.\n\n\nWhat is trend of age in each season?\nThis is a fun question, let‚Äôs try to find the slope of the best fit line across the previous plots. This can be achieved by implementing the equation of the slope of single variable linear regression: \\[b= \\frac{n\\sum{xy} - \\sum{x}\\sum{y}}{n\\sum{x^2}-(\\sum{x})^2}\\]\n\n%%sql\nselect (n*SigmaXY - SigmaX * SigmaY)/(n*SigmaX2 - SigmaX^2) as b from (\n                select cast(Sum(Y) as float) as SigmaY, \n                       sum(X^2) as SigmaX2,\n                       sum(X) as SigmaX,\n                       sum(Y*X) as SigmaXY,\n                       count(*) as n\n                       from (\n                        SELECT Year-1896 as X, \n                               AVG(cast(Age as int)) as Y \n                        FROM   athlete_events\n                        WHERE Age is not NULL \n                              AND Age &lt;&gt; 'NA'\n                              AND Sport &lt;&gt; 'Art Competitions'\n                              AND Season = 'Summer'\n                        GROUP BY Year \n                ) t1 \n) t2\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nb\n\n\n-0.4668425612349374\n\n\n\n\n\n\n%%sql\nSELECT ( n * sigmaxy - sigmax * sigmay ) / ( n * sigmax2 - sigmax^2 ) AS b\nFROM   (SELECT Cast(Sum(y) AS FLOAT) AS SigmaY,\n               Sum(x^2)              AS SigmaX2,\n               Sum(x)                AS SigmaX,\n               Sum(y * x)            AS SigmaXY,\n               Count(*)              AS n\n        FROM   (SELECT year - 1896           AS X,\n                       Avg(Cast(age AS INT)) AS Y\n                FROM   athlete_events\n                WHERE  age IS NOT NULL\n                       AND age &lt;&gt; 'NA'\n                       AND sport &lt;&gt; 'Art Competitions'\n                       AND season = 'Winter'\n                GROUP  BY year) t1) t2 \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nb\n\n\n-0.13860606401530068\n\n\n\n\n\nWe can see a stronger negative trend in summer events than in winter events.\n\n\nWhat is the number of participating nations for each year and season?\n\n%%sql result &lt;&lt;\nSELECT *\nFROM   (SELECT DISTINCT Year,\n                        Season,\n                        region\n        FROM   athlete_events\n               LEFT JOIN noc_regions\n                      ON athlete_events.NOC = noc_regions.NOC) t\n       PIVOT ( Count(region)\n             FOR season IN (Summer,\n                            Winter) ) piv\nORDER  BY Year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable result\n\n\n\nresult_df = result.DataFrame()\nresult_df.plot.bar(x='Year', y =['Summer', 'Winter'], title='# Participating Nations The Olympics')\n\n&lt;AxesSubplot:title={'center':'# Participating Nations The Olympics'}, xlabel='Year'&gt;\n\n\n\n\n\nFew observation: * The number of participating nations has been increasing since the inception of the first Olympic season in 1896. * The first winter Olympics was held in 1924. * Less nations participate in the winter Olympics as compared with the summer ones. * Since 1992, the Olympics are held every two years for alternating seasons.\n\n\nIn which years and seasons did Iraq not participate in the Olympics?\n\n%%sql\nSELECT Year,\n       Season,\n       Sum(Iraq) AS IraqIn\nFROM   (SELECT DISTINCT Year,\n                        Season,\n                        region,\n                        CASE region\n                          WHEN 'Iraq' THEN 1\n                          ELSE 0\n                        END AS Iraq\n        FROM   athlete_events\n               LEFT JOIN noc_regions\n                      ON athlete_events.NOC = noc_regions.NOC) t\nGROUP  BY Year,\n          Season\nHAVING Sum(Iraq) = 0\nORDER  BY Year \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nYear\nSeason\nIraqIn\n\n\n1896\nSummer\n0\n\n\n1900\nSummer\n0\n\n\n1904\nSummer\n0\n\n\n1906\nSummer\n0\n\n\n1908\nSummer\n0\n\n\n1912\nSummer\n0\n\n\n1920\nSummer\n0\n\n\n1924\nWinter\n0\n\n\n1924\nSummer\n0\n\n\n1928\nWinter\n0\n\n\n1928\nSummer\n0\n\n\n1932\nWinter\n0\n\n\n1932\nSummer\n0\n\n\n1936\nWinter\n0\n\n\n1936\nSummer\n0\n\n\n1948\nWinter\n0\n\n\n1952\nWinter\n0\n\n\n1952\nSummer\n0\n\n\n1956\nWinter\n0\n\n\n1956\nSummer\n0\n\n\n1960\nWinter\n0\n\n\n1964\nWinter\n0\n\n\n1968\nWinter\n0\n\n\n1972\nSummer\n0\n\n\n1972\nWinter\n0\n\n\n1976\nWinter\n0\n\n\n1976\nSummer\n0\n\n\n1980\nWinter\n0\n\n\n1984\nWinter\n0\n\n\n1988\nWinter\n0\n\n\n1992\nWinter\n0\n\n\n1994\nWinter\n0\n\n\n1998\nWinter\n0\n\n\n2002\nWinter\n0\n\n\n2006\nWinter\n0\n\n\n2010\nWinter\n0\n\n\n2014\nWinter\n0\n\n\n\n\n\n\n\nWhat is the number of medals per Sport for sports which have more than 500 medals granted?\n\n%%sql results &lt;&lt;\nSELECT Sport,\n       COUNT(Medal) AS MedalsGranted\nFROM   (SELECT DISTINCT CAST(athlete_events.NAME AS CHAR(100)) AS Name,\n                        Games,\n                        region,\n                        Sport,\n                        Medal\n        FROM   athlete_events\n               LEFT JOIN noc_regions\n                      ON athlete_events.NOC = noc_regions.NOC\n        WHERE  Medal IS NOT NULL\n               AND Medal &lt;&gt; 'NA') t1\nGROUP  BY Sport\nHAVING COUNT(Medal) &gt; 500\nORDER  BY MedalsGranted DESC\n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable results\n\n\n\nresults_df = results.DataFrame()\nresults_df.plot.bar(x='Sport', y='MedalsGranted')\n\n&lt;AxesSubplot:xlabel='Sport'&gt;\n\n\n\n\n\n\n\nWhich countries participated less than 5 times in the Olympics?\n\n%%sql\nSELECT region,\n       Count(Games) AS TimesParticipated\nFROM   (SELECT DISTINCT Games,\n                        region\n        FROM   athlete_events\n               LEFT JOIN noc_regions\n                      ON athlete_events.NOC = noc_regions.NOC\n        WHERE  region IS NOT NULL\n               AND region &lt;&gt; 'NA') t1\nGROUP  BY region\nHAVING Count(Games) &lt; 5\nORDER  BY TimesParticipated \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nregion\nTimesParticipated\n\n\nKosovo\n1\n\n\nSouth Sudan\n1\n\n\nMarshall Islands\n3\n\n\nKiribati\n4\n\n\n\n\n\n\n\nWhich are the top 5 countries by the number of Judo players?\n\n%%sql\nSELECT TOP 5 region,\n       Count(DISTINCT Cast(Name AS NVARCHAR(50))) NumPlayers\nFROM   athlete_events\n       LEFT JOIN noc_regions\n              ON athlete_events.NOC = noc_regions.NOC\nWHERE  Sport = 'Judo'\nGROUP  BY region\nORDER  BY NumPlayers DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nregion\nNumPlayers\n\n\nGermany\n95\n\n\nSouth Korea\n95\n\n\nRussia\n94\n\n\nFrance\n93\n\n\nJapan\n93\n\n\n\n\n\n\n\nWhat is percentage of Judo players in each of these countries?\n\n%%sql result &lt;&lt;\nSELECT top 20 region,\n       ( CAST(COUNT(DISTINCT CAST(Name AS NVARCHAR(50))) AS FLOAT) /\n           (SELECT COUNT(DISTINCT CAST(Name AS NVARCHAR(50))) NumJudoPlayers\n        FROM\n           athlete_events\n        WHERE\n           sport =\n           'Judo'\n           ) ) * 100 JudoPlayersPercentage\nFROM   athlete_events\n       LEFT JOIN noc_regions\n              ON athlete_events.noc = noc_regions.noc\nWHERE  sport = 'Judo'\nGROUP  BY region\nORDER  BY JudoPlayersPercentage DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\nReturning data to local variable result\n\n\n\nresult_df = result.DataFrame()\nresult_df.plot.bar(x='region', y='JudoPlayersPercentage')\n\n&lt;AxesSubplot:xlabel='region'&gt;\n\n\n\n\n\n\n\nWhich are the top 5 countries by the number of medals in judo?\n\n%%sql\nSELECT TOP 5 region,\n             Count(Medal) AS NumMedals\nFROM   (SELECT DISTINCT CAST(NAME AS CHAR(100)) AS Name,\n                        Games,\n                        region,\n                        Medal\n        FROM   athlete_events\n               LEFT JOIN noc_regions\n                      ON athlete_events.NOC = noc_regions.NOC\n        WHERE  sport = 'Judo'\n               AND Medal &lt;&gt; 'NA') t1\nGROUP  BY region\nORDER  BY NumMedals DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nregion\nNumMedals\n\n\nJapan\n84\n\n\nFrance\n49\n\n\nRussia\n43\n\n\nSouth Korea\n43\n\n\nGermany\n37\n\n\n\n\n\n\n\nWho are the top 5 players who participated the largest number of times?\n\n%%sql\nSELECT TOP 5 CAST(NAME AS CHAR(100)) Name,\n             COUNT(Games)            GamesParticipated\nFROM   athlete_events\nWHERE Sport &lt;&gt; 'Art Competitions'\nGROUP  BY CAST(NAME AS CHAR(100))\nORDER  BY GamesParticipated DESC \n\n * mssql+pyodbc://@localhost\\SQLEXPRESS/OlympicsHistory?driver=SQL+Server&trusted_connection=yes\nDone.\n\n\n\n\n\nName\nGamesParticipated\n\n\nHeikki Ilmari Savolainen\n39\n\n\nJoseph \"Josy\" Stoffel\n38\n\n\nIoannis Theofilakis\n36\n\n\nTakashi Ono\n33\n\n\nAndreas Wecker\n32"
  },
  {
    "objectID": "posts/D3js-Explorations-4/index.html",
    "href": "posts/D3js-Explorations-4/index.html",
    "title": "D3 - Visualizing temperature 2",
    "section": "",
    "text": "Today‚Äôs post is a bit delayed, anyway, I went through the same data from day 1 (temperature measurements) but this time I, used leaflet.js instead of google maps API starting from from the example. The beautiful thing about leaflet is that it doesn‚Äôt abstract the whole DOM event-handling like in google maps which means events set using d3 are handled normally.\nI have manged to encoded the data in color, size and in text in the form of tool-tips.\n\nThe code can be found at here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Interrogator",
    "section": "",
    "text": "A little trip down the asset management lane\n\n\n\n\n\n\n\nAsset Management\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nCrop classification in Satellite imagry\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nLearning Analytics - Dashboard with Google Data Studio\n\n\n\n\n\n\n\ndata viz\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2021\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nA solution to a coding challenge\n\n\n\n\n\n\n\nPython\n\n\nDocker\n\n\nSoftware Engineering\n\n\nData Structures\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nEfficient Data Analysis - SQL and Python\n\n\n\n\n\n\n\nPython\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2021\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nMaking the TITAN GTX GPU available to Tensorflow\n\n\n\n\n\n\n\nPython\n\n\nLinux\n\n\nTensorflow\n\n\nSoftware Engineering\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nID Mapping using Selenium\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\nSelenium\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2019\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nBenford‚Äôs Law\n\n\n\n\n\n\n\nanalysis\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2017\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nD3 - Visualizing temperature 2\n\n\n\n\n\n\n\ndata viz\n\n\nd3\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2017\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nD3 - Visualizing connections\n\n\n\n\n\n\n\ndata viz\n\n\nd3\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2017\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nD3 - Visualizing bike share stations on map\n\n\n\n\n\n\n\ndata viz\n\n\nd3\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2017\n\n\nWaseem Waheed\n\n\n\n\n\n\n  \n\n\n\n\nD3 - Visualizing temperature 1\n\n\n\n\n\n\n\ndata viz\n\n\nd3\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2017\n\n\nWaseem Waheed\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there, I‚Äôm Waseem Waheed. By day, I dive into the world of Asset Management Engineering at VicTrack, where I‚Äôm constantly uncovering opportunities for improvement and making them a reality.\nBut that‚Äôs just part of my story. As a Sessional Academic at La Trobe University, I get to share my passion for various subjects like Natural Language Processing, Data Mining, Internet of Things, and Wireless Communications/Networking. It‚Äôs incredibly rewarding to inspire and guide students in these dynamic fields.\nAt the core of my journey lies a PhD in Image Processing, delving deep into the realm of continuous optimization techniques within signal and image processing. My research spans across Optimization, Signal and Image Processing, and Graph Theory, where I explore the intricate connections between these domains.\nMy curiosity doesn‚Äôt end there. I‚Äôm captivated by the vast landscape of data science, computer vision, natural language processing, programming, networking, and all things IT-related.\nHere‚Äôs a snapshot of what I bring to the table:\n\nProficiency in Python for software engineering\nHarnessing R and Python for predictive analytics\nDiving into data analysis using SQL, R, Python, and even Excel\nCrafting compelling insights through data visualization with PowerBI\nAnd let‚Äôs not forget the art of technical writing\n\nJoin me on this journey where technology meets innovation, and let‚Äôs explore the endless possibilities together."
  },
  {
    "objectID": "posts/Making-TITAN-GTX-GPU-available-to-Tensorflow/index.html",
    "href": "posts/Making-TITAN-GTX-GPU-available-to-Tensorflow/index.html",
    "title": "Making the TITAN GTX GPU available to Tensorflow",
    "section": "",
    "text": "Introduction\nThis post is to document üìù a process I had to go through. Installing Tensorflow on Ubuntu.\nTo make use of the GPU(s) in Tensorflow, Linux should be able to detect the GPU and tensorflow-gpu should be installed.\n\n\nThe process\nFirst of all, you need to check that Linux can detect the GPU as follows: you can either do:\nuser# sudo lshw -C display\nor:\nuser# nvidia-smi\nwhich should show you a list containing all the NVIDIA GPUs attached to your machine.\nNext, to check if tensorflow-gpu is installed, we can use the following command\nuser# pip list | grep tensor\ntensorboard                       2.4.1\ntensorboard-plugin-wit            1.8.0\ntensorflow                        1.14.0\ntensorflow-estimator              2.3.0\nas we can see, tensorflow-gpu is not in the list. To install it, we use:\nuser# sudo pip install --upgrade pip\nuser# pip install tensorflow-gpu --user\nonce finished, we should check the correct operation of Tensorflow as follows:\nuser# python\nimport tensorflow as tf\nprint(tf.config.experimental.list_physical_devices('GPU'))\nwhich should give you a list of the installed GPUs. To use a specific GPU within a context:\ntf.debugging.set_log_device_placement(True)\n\ntry:\n  # Specify an invalid GPU device\n  with tf.device('/device:GPU:1'):\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    c = tf.matmul(a, b)\nexcept RuntimeError as e:\n  print(e)\nTo use a specific GPU for all Tensorflow‚Äôs calculations, use the following template:\nimport tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\nNote: this note took advantage of Tensorflow‚Äôs documentation @ https://www.tensorflow.org/guide/gpu\n\n\nIn the end\nI hope that this write up has helped you installing from scratch/fixing your Tensorflow installation."
  },
  {
    "objectID": "posts/Benfords-Law/index.html",
    "href": "posts/Benfords-Law/index.html",
    "title": "Benford‚Äôs Law",
    "section": "",
    "text": "Benford‚Äôs law\nFew days ago, I was introduced to the Benford‚Äôs law of distribution. Looking up the internet for accessible explanation, I came across a basic introduction with very interesting explanation on Khan Academy. Sal mentioned the typical example of the world countries populations follow this fascinating distribution function.\nAfter few seconds of thinking about it, I decided to confirm this claim, So, I quickly pulled the data from wikipedia and coded a quick and dirty experiment. And sure it works as you can see bellow\n\nCode and data can be found in my GitHub"
  },
  {
    "objectID": "posts/D3js-Explorations-1/index.html",
    "href": "posts/D3js-Explorations-1/index.html",
    "title": "D3 - Visualizing temperature 1",
    "section": "",
    "text": "This is the first day of the challenge but I have been tinkering with D3 for few days now.\nMy first day started with an intention to project points on a map so, I had a look at few examples by they seemed complicated so I adjusted my aims from drawing the map with d3 to mapping the data with d3 and get the map from a map provider (google maps or leaflet). I started with an example by Mike Bostock which is very accessible and I adapted it. My data that I had which is the locations of temperature sensors spread across the US (source : GSOD) as well as summary temperature measurement for one day. I‚Äôm happy with the result for today as I managed to project the sensors on the map and color coded the points based on temperature measurement.\nMy adapted version of the code can be found on my github\n\nFuture directions : 1. Draw connections between nodes 2. include a slider to select the day of the year."
  },
  {
    "objectID": "posts/A-little-trip-down-the-asset-management-lane/index.html",
    "href": "posts/A-little-trip-down-the-asset-management-lane/index.html",
    "title": "A little trip down the asset management lane",
    "section": "",
    "text": "The purpose of this post is twofold ‚úåÔ∏è; document my learnings üìù about what asset management is, and the role I played recently in a gig with asset management and data analytics.\n\n\nI will by qualifying what is considered an asset:\n\nPlant üè≠(chiller, boiler, compressor, generator, tracks, cabling, racks, piping, etc.).\nEquipment üñ•Ô∏è(computer, router, switch, CT scanner, X-ray machines, MRI machines, etc.).\nBuildings üè®.\nIntellectual property ¬© (Know-how, patents, copyrights, designs, etc.).\nSoftware licenses üìÄ.\n\nNote: I have intentionally omitted financial assets, as they fall outside the scope of my work.\nAssets have life-cycles within organizations, which go through four phases: 1. Planning üìù, 2. Acquisition üõí, 3. Operation üèÉ, 4. Disposal ‚ôªÔ∏è.\nAsset management, as defined by the standard ISO 50000, defines a systematic approach for managing assets with the following goals:\n\nReducing the cost üí≤ across the life-cycle of the asset.\nReduce risk ‚ö†Ô∏è and keep it under agreed and acceptable levels.\nMeet service level agreements (performance).\nMeet organizational sustainability obligations.\n\nIn other words, asset management defines processes and procedures for optimizing the return-on-investment (ROI) üí∞ from assets across the four phases of their life-cycles.\nThe following diagram outlines some of the processes at each phase.\n\n\n\nOperational asset management defines processes and procedures for assets in operation. Monitoring is one of the processes, it defines the method of monitoring, the parameters to be captured, the frequency of information capture, the location of information, etc.\nMaintenance is another process or set of processes, it defines the maintenance procedure, frequency, thresholds, records location, etc.\n\n\n\nThe Asset Management Accountability Framework (AMAF) is an important tool for managing assets, developed by the Department of Treasury and Finance (DTF) of Victoria State Government for the public sector. This framework outlines mandatory requirements and general guidance for public sector organizations to effectively manage their assets and meet service level obligations to the public.\nFurther information about AMAF can be found here\n\n\n\n\nIt should be obvious by now that data is going to be collected across the life-cycle of assets to facilitate decision-making towards achieving the asset management objectives.\n\n\nAmong the data analytics techniques that can be used in for asset management include:\n‚úî Data QA,\n‚úî Data modeling,\n‚úî Summary statistics,\n‚úî Basic algebra,\n‚úî Understanding of metricsüìè,\n‚úî Time-series analysis and prediction,\n‚úî Anomaly detectionüôÉ,\n‚úî Monte Carlo simulation üé≤,\n‚úî Data visualization üìà and dashboard development.\n\n\n\nIn a recent gig, I was presented with an open-ended problem in which a business was struggling to keep up with the monitoring of a particular type of asset due to the lack of accurate and centralized data.\nThe business owned ~1200 units of this asset spread across ~600 sites and asset state monitoring was conducted once every year. To allow for accurate scheduling of the monitoring program, they need to have an accurate asset register.\nThe organization was in the process of establishing a centralized asset register, with data primarily captured in spreadsheets by various teams. This presented challenges for monitoring technicians, who would often discover newly installed assets during on-site monitoring visits. This disrupted the monitoring schedule, as the organization was undergoing a period of growth and new assets were being introduced frequently.\nIn summary, newly installed assets were not documented anywhere for the monitoring team and other teams to manage properly.\nIf you are thinking wow, data about the asset is being captured once every year! I can tell you that it is acceptable for this particular asset type, given its role and criticality.\n\n\n\nAfter a lot of consultations with various stakeholders across the organization, I was able to gather a clearer picture of the process, enabling me to pinpoint where it was failing.\nIf you have been following along, you should be able to discern that the problem is that the data was not centralized and communication between commissioning and monitoring teams was broken.\nThe solution to this problem is to centralize the data and modify the process to enable the commissioning team to add newly deployed assets. That is easier said than done ü§≠.\nThe project face numerous challenges including:\n\nMonitoring data was not normalized and didn‚Äôt adhere to a unified data model. It was stored in one big jungle of a spreadsheet. Understanding how it worked was a challenge, let alone extracting the data.\nInternal resistance to change.\nTime limitation.\n\nTo be able to deliver an improvement, I had to split the project into phases, outlining what can be expected at the end of each phase. These phases had to avoid introducing any breaking changes to the process.\nBy the end of the time allocated to the project, I was able to finish the first phase, which delivered:\n\na centralized spreadsheet,\nwith a data model,\nand instructions on how data should be captured to guarantee uniqueness and consistency.\nA working process that guarantees recency in terms of the deployed assets.\nA documentation of what the next phase should deliver.\n\n\n\n\n\nAmong the skills, working on this project has highlighted are the:\n\nAbility to manage change ‚ú®, this goes beyond basic communications. People are going to resist change, and communicating change with people at various levels of the hierarchy is a skill in its own right.\nAbility to be work with minimal guidance üî• and be a self-starter. What to do next is not always clear.\nWorking collaboratively ü§ùüèæ with others.\nAttention to detail üîé.\nTechnical skills üë©üèª‚Äçüíª, including intermediate to advanced spreadsheet skills, data modeling, scripting, and data visualization.\n\nüöÄ If you‚Äôre someone who‚Äôs passionate about asset management, engineering, or IT, have you ever considered turning your expertise into a career in data analytics? Now is the perfect time to make the switch. Companies are rapidly digitizing and they‚Äôre on the lookout for people like you who can plan and develop data collection and reporting."
  },
  {
    "objectID": "posts/A-little-trip-down-the-asset-management-lane/index.html#asset-management",
    "href": "posts/A-little-trip-down-the-asset-management-lane/index.html#asset-management",
    "title": "A little trip down the asset management lane",
    "section": "",
    "text": "I will by qualifying what is considered an asset:\n\nPlant üè≠(chiller, boiler, compressor, generator, tracks, cabling, racks, piping, etc.).\nEquipment üñ•Ô∏è(computer, router, switch, CT scanner, X-ray machines, MRI machines, etc.).\nBuildings üè®.\nIntellectual property ¬© (Know-how, patents, copyrights, designs, etc.).\nSoftware licenses üìÄ.\n\nNote: I have intentionally omitted financial assets, as they fall outside the scope of my work.\nAssets have life-cycles within organizations, which go through four phases: 1. Planning üìù, 2. Acquisition üõí, 3. Operation üèÉ, 4. Disposal ‚ôªÔ∏è.\nAsset management, as defined by the standard ISO 50000, defines a systematic approach for managing assets with the following goals:\n\nReducing the cost üí≤ across the life-cycle of the asset.\nReduce risk ‚ö†Ô∏è and keep it under agreed and acceptable levels.\nMeet service level agreements (performance).\nMeet organizational sustainability obligations.\n\nIn other words, asset management defines processes and procedures for optimizing the return-on-investment (ROI) üí∞ from assets across the four phases of their life-cycles.\nThe following diagram outlines some of the processes at each phase.\n\n\n\nOperational asset management defines processes and procedures for assets in operation. Monitoring is one of the processes, it defines the method of monitoring, the parameters to be captured, the frequency of information capture, the location of information, etc.\nMaintenance is another process or set of processes, it defines the maintenance procedure, frequency, thresholds, records location, etc.\n\n\n\nThe Asset Management Accountability Framework (AMAF) is an important tool for managing assets, developed by the Department of Treasury and Finance (DTF) of Victoria State Government for the public sector. This framework outlines mandatory requirements and general guidance for public sector organizations to effectively manage their assets and meet service level obligations to the public.\nFurther information about AMAF can be found here"
  },
  {
    "objectID": "posts/A-little-trip-down-the-asset-management-lane/index.html#data-analytics",
    "href": "posts/A-little-trip-down-the-asset-management-lane/index.html#data-analytics",
    "title": "A little trip down the asset management lane",
    "section": "",
    "text": "It should be obvious by now that data is going to be collected across the life-cycle of assets to facilitate decision-making towards achieving the asset management objectives.\n\n\nAmong the data analytics techniques that can be used in for asset management include:\n‚úî Data QA,\n‚úî Data modeling,\n‚úî Summary statistics,\n‚úî Basic algebra,\n‚úî Understanding of metricsüìè,\n‚úî Time-series analysis and prediction,\n‚úî Anomaly detectionüôÉ,\n‚úî Monte Carlo simulation üé≤,\n‚úî Data visualization üìà and dashboard development.\n\n\n\nIn a recent gig, I was presented with an open-ended problem in which a business was struggling to keep up with the monitoring of a particular type of asset due to the lack of accurate and centralized data.\nThe business owned ~1200 units of this asset spread across ~600 sites and asset state monitoring was conducted once every year. To allow for accurate scheduling of the monitoring program, they need to have an accurate asset register.\nThe organization was in the process of establishing a centralized asset register, with data primarily captured in spreadsheets by various teams. This presented challenges for monitoring technicians, who would often discover newly installed assets during on-site monitoring visits. This disrupted the monitoring schedule, as the organization was undergoing a period of growth and new assets were being introduced frequently.\nIn summary, newly installed assets were not documented anywhere for the monitoring team and other teams to manage properly.\nIf you are thinking wow, data about the asset is being captured once every year! I can tell you that it is acceptable for this particular asset type, given its role and criticality.\n\n\n\nAfter a lot of consultations with various stakeholders across the organization, I was able to gather a clearer picture of the process, enabling me to pinpoint where it was failing.\nIf you have been following along, you should be able to discern that the problem is that the data was not centralized and communication between commissioning and monitoring teams was broken.\nThe solution to this problem is to centralize the data and modify the process to enable the commissioning team to add newly deployed assets. That is easier said than done ü§≠.\nThe project face numerous challenges including:\n\nMonitoring data was not normalized and didn‚Äôt adhere to a unified data model. It was stored in one big jungle of a spreadsheet. Understanding how it worked was a challenge, let alone extracting the data.\nInternal resistance to change.\nTime limitation.\n\nTo be able to deliver an improvement, I had to split the project into phases, outlining what can be expected at the end of each phase. These phases had to avoid introducing any breaking changes to the process.\nBy the end of the time allocated to the project, I was able to finish the first phase, which delivered:\n\na centralized spreadsheet,\nwith a data model,\nand instructions on how data should be captured to guarantee uniqueness and consistency.\nA working process that guarantees recency in terms of the deployed assets.\nA documentation of what the next phase should deliver."
  },
  {
    "objectID": "posts/A-little-trip-down-the-asset-management-lane/index.html#reflection",
    "href": "posts/A-little-trip-down-the-asset-management-lane/index.html#reflection",
    "title": "A little trip down the asset management lane",
    "section": "",
    "text": "Among the skills, working on this project has highlighted are the:\n\nAbility to manage change ‚ú®, this goes beyond basic communications. People are going to resist change, and communicating change with people at various levels of the hierarchy is a skill in its own right.\nAbility to be work with minimal guidance üî• and be a self-starter. What to do next is not always clear.\nWorking collaboratively ü§ùüèæ with others.\nAttention to detail üîé.\nTechnical skills üë©üèª‚Äçüíª, including intermediate to advanced spreadsheet skills, data modeling, scripting, and data visualization.\n\nüöÄ If you‚Äôre someone who‚Äôs passionate about asset management, engineering, or IT, have you ever considered turning your expertise into a career in data analytics? Now is the perfect time to make the switch. Companies are rapidly digitizing and they‚Äôre on the lookout for people like you who can plan and develop data collection and reporting."
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html",
    "href": "posts/ML-Challenge/ML-Challenge.html",
    "title": "Crop classification in Satellite imagry",
    "section": "",
    "text": "As part of my ML practice, I was sent an ML challenge to develope a classifier that classfies pixels of hyperspectral images into various crops, which is an instance of a common problem known as crop mapping.\nThe satellite images in this instance are captured by the Sentinel-2 satellite.\nSentinel-2 is an earth observation mission by the European Space Agency."
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#mean-values-and--2-standard-deviations-of-all-the-bands-per-crop",
    "href": "posts/ML-Challenge/ML-Challenge.html#mean-values-and--2-standard-deviations-of-all-the-bands-per-crop",
    "title": "Crop classification in Satellite imagry",
    "section": "3. Mean values and +-2 standard deviations of all the bands per crop",
    "text": "3. Mean values and +-2 standard deviations of all the bands per crop\n\n3.1 Other\nTo plot the \\(‚àì2œÉ\\) standard deviations, we can plot box plots with the whiskers of the boxes at the \\(‚àì2œÉ\\) by setting the whiskers to the percentiles 2.28, 97.72 Source\n\n\nCode\ndef plot_means_and_stds(data):\n  ax, bp = data[bands].boxplot(whis=[2.28, 97.72], vert=False, showmeans=True, figsize=(15,15), return_type=\"both\")\n  means = data[bands].mean(axis=0)\n  stds = data[bands].std(axis=0)\n  for i, line in enumerate(bp['medians']):\n      x, y = line.get_xydata()[1]\n      text = '-2œÉ={:.2f}, Œº={:.2f}, +2œÉ={:.2f}'.format(means[i]-2*stds[i], means[i], means[i]+2*stds[i])\n      ax.annotate(text, xy=(max(0, x-0.07), y+0.07))\n  ax.set_xlim(0, 1.1)\n\n\n\n\nCode\ndata_other = data_no_clouds[data_no_clouds['label'] == 'Other']\nplot_means_and_stds(data_other)\n\n\n\n\n\n\n\n3.2 Sorghum\n\n\nCode\ndata_sorghum = data_no_clouds[data_no_clouds['label'] == 'Sorghum']\nplot_means_and_stds(data_sorghum)\n\n\n\n\n\n\n\n3.3 Cotton\n\n\nCode\ndata_cotton = data_no_clouds[data_no_clouds['label'] == 'Cotton']\nplot_means_and_stds(data_cotton)\n\n\n\n\n\nThis figure is promising as the values in the bands B6-B9 seem to be distinguishing features for the cotton crop."
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#machine-learning-model-training",
    "href": "posts/ML-Challenge/ML-Challenge.html#machine-learning-model-training",
    "title": "Crop classification in Satellite imagry",
    "section": "4. Machine learning model training",
    "text": "4. Machine learning model training\n\nStep 1: Baseline model: Multiclass Logistic Regression\nThe first model I am going to try is the multiclass logistic regression for it‚Äôs simplicity to get an idea about the possible accuracy.\n\n\nCode\ndef train_test_assess(data, model):\n    X = data[data.columns[~data.columns.isin(['label_id', 'label', 'cloud_prob'])]]\n    y = data['label_id']\n    labels = [ids2labels[id] for id in sorted(data['label_id'].unique().tolist())]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_test)\n    print('Accuracy: {:.2f}%'.format(accuracy_score(y_predicted, y_test)*100 ))\n    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, display_labels=labels)\n    # if binary classification, report the AUC metric\n    if len(labels) == 2:\n        fpr, tpr, thresholds = roc_curve(y_test==1, y_predicted==1, pos_label=1)\n        print('AUC: {:.2f}'.format(auc(fpr, tpr)))\n    return model\n\n\n\n\nCode\nclf = LogisticRegression(max_iter=10000, multi_class='multinomial')\nclf = train_test_assess(data_no_clouds, clf)\n\n\nAccuracy: 95.48%\n\n\n\n\n\nThis initial result is consistent with our initial intuition, Cotton is easier classify than Other and Sorghum.\nIt might a good idea to search the literature at this point for discriminative features.\nI‚Äôm going to attempt to try to focus on the Sorghum vs Other problem next.\n\n\nStep 2: Binary classification (Sorghum vs Other)\n\n\nCode\ndata_bin = data_no_clouds[~data_no_clouds['label'].isin(['Cotton'])]\n\n\n\n\nCode\ndata_bin['label'].value_counts()/data_bin.shape[0] * 100\n\n\nOther      95.324246\nSorghum     4.675754\nName: label, dtype: float64\n\n\nThe data is highly imbalanced, as a first attempt to fix that, let‚Äôs try to add weights.\n\n\nStep 3: Binary classification with Random Forest\n\nStep 3.1: Binary classification with Random Forest and Remote Sensing\nNext, we test the benefit of adding NDVI and NDWI indices\nNDVI: The normalized difference vegetation index, an effective index for quantifying green vegetation.\nNDWI: The normalized difference water index used to monitor changes related to water content in water bodies.\nNDMI: The normalized difference moisture index used to monitor changes in water content of leaves.\nFor a full list of remote sensing indices https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/indexdb/\n\n\nCode\ndata_bin_aug = data_bin.copy()\n\n# Calculate NDVI according to https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndvi/\ndata_bin_aug['NDVI'] = (data_bin['B08'] - data_bin['B04'])/(data_bin['B08'] + data_bin['B04'])\n# Calculate NDWI according to https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/\ndata_bin_aug['NDWI'] = (data_bin['B03'] - data_bin['B08'])/(data_bin['B03'] + data_bin['B08'])\n# Calculate NDMI according to https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndmi/\ndata_bin_aug['NDMI'] = (data_bin['B08'] - data_bin['B11'])/(data_bin['B08'] + data_bin['B11'])\n# Calculate NDMI according to https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/gndvi/#\ndata_bin_aug['GNDVI'] = (data_bin['B08'] - data_bin['B03'])/(data_bin['B08'] + data_bin['B03'])\n\n# source : https://eprints.lancs.ac.uk/id/eprint/136586/2/Author_Accepted_Manuscript.pdf\ndata_bin_aug['GCVI'] = (data_bin['B8A'] / data_bin['B03']) - 1\ndata_bin_aug['SR'] = data_bin['B8A'] / data_bin['B04']\ndata_bin_aug['WDRVI'] = (0.2*data_bin['B8A'] - data_bin['B04'])/(0.2*data_bin['B8A'] + data_bin['B04'])\n\n\n\n\n\nStep 4: Random under- and over-sampling\nTo solve the data imbalance issue, let‚Äôs use resampling techinques.\nFirst we are going to start by under-sampling, that is to randomly pick samples from the majority group such that the majority and minority classes become of the same size.\n\n\nCode\n# Features importances\ndef calc_feats_importances(model, data):\n    imps = list(model.feature_importances_/sum(model.feature_importances_) * 100)\n    feats = data.columns[~data.columns.isin(['label_id', 'label', 'cloud_prob'])]\n    feats_imps = dict(zip(feats, imps))\n    return feats_imps\n\n\nUndersampling\n\n\nCode\n# Class count\nprint('Class distribution: \\n'+str(data_bin['label'].value_counts()))\n\nclf_bin5 = make_pipeline_with_sampler(\n    RandomUnderSampler(),\n    RandomForestClassifier(),\n)\n\nclf_bin5 = train_test_assess(data_bin_aug, clf_bin5)\ncalc_feats_importances(clf_bin5[1], data_bin_aug)\n\n\nClass distribution: \nOther      272879\nSorghum     13385\nName: label, dtype: int64\nAccuracy: 93.74%\nAUC: 0.96\n\n\n{'B01': 16.977792737026085,\n 'B02': 6.752882418570922,\n 'B03': 2.554969210163749,\n 'B04': 2.6563337118232186,\n 'B05': 3.3259631537606507,\n 'B06': 3.9223474314521467,\n 'B07': 3.8106562700819713,\n 'B08': 3.5390293533814705,\n 'B8A': 4.125422731372653,\n 'B09': 11.247041256299674,\n 'B11': 6.183133344698969,\n 'B12': 8.360377618059259,\n 'NDVI': 3.579062573810352,\n 'NDWI': 4.239217235594501,\n 'NDMI': 4.276727881317791,\n 'GNDVI': 4.767229788099875,\n 'GCVI': 3.348787865016853,\n 'SR': 3.351913622009852,\n 'WDRVI': 2.9811117974600068}\n\n\n\n\n\nSince features B03, B04, B05, B07, B08 seem to have the least effect, let‚Äôs try removing them\n\n\nCode\n# Class count\nclf_bin6 = make_pipeline_with_sampler(\n    RandomUnderSampler(),\n    RandomForestClassifier(),\n)\n# remove the less important features\ndata_bin_imp = data_bin_aug.copy()\ndata_bin_imp = data_bin_imp[data_bin_imp.columns[~data_bin_imp.columns.isin(['B03', 'B04', 'B05', 'B07' ,'B08'])]]\n\nclf_bin6 = train_test_assess(data_bin_imp, clf_bin6)\ncalc_feats_importances(clf_bin6[1], data_bin_imp)\n\n\nAccuracy: 93.56%\nAUC: 0.95\n\n\n{'B01': 18.12897209994457,\n 'B02': 7.237858677151985,\n 'B06': 6.580060537808315,\n 'B8A': 6.095497050380915,\n 'B09': 12.43070752907386,\n 'B11': 8.321135912566861,\n 'B12': 9.327024620656,\n 'NDVI': 4.785401272622748,\n 'NDWI': 4.975197715869764,\n 'NDMI': 5.109331237453354,\n 'GNDVI': 5.734742204540842,\n 'GCVI': 4.490255452417144,\n 'SR': 3.3861633080797793,\n 'WDRVI': 3.3976523814338675}\n\n\n\n\n\nNext, I will test the accuracy on the full data (including Sorghum)\n\n\nCode\nclf8 = make_pipeline_with_sampler(\n    RandomUnderSampler(),\n    RandomForestClassifier(),\n)\n\nclf8 = train_test_assess(data_no_clouds, clf8)\ncalc_feats_importances(clf8[1], data_no_clouds)\n\n\nAccuracy: 94.46%\n\n\n{'B01': 9.6464853828485,\n 'B02': 9.714544169136957,\n 'B03': 2.7639361993439553,\n 'B04': 8.411027820590892,\n 'B05': 2.6418393007109287,\n 'B06': 4.068313242872104,\n 'B07': 12.051608284154835,\n 'B08': 18.782597265383384,\n 'B8A': 12.360924465779007,\n 'B09': 8.479810816147658,\n 'B11': 4.532715522991699,\n 'B12': 6.546197530040068}\n\n\n\n\n\nI am going to pick the model with RandomDownSampler as it gives satisfactory results for the current data, the next step is a feature selection process.\n\n\nCode\nX = data_bin_aug[data_bin_aug.columns[~data_bin_aug.columns.isin(['label_id', 'label', 'cloud_prob'])]]\ny = data_bin_aug['label_id']\n\n\n\n\nCode\n# source: https://imbalanced-learn.org/stable/under_sampling.html\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler \n\nprint(sorted(Counter(y).items()))\ncc = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = cc.fit_resample(X, y)\nprint(sorted(Counter(y_resampled).items()))\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3)\n\n\n[(0, 272879), (1, 13385)]\n[(0, 13385), (1, 13385)]\n\n\nNext, we perform feature selection to find the most important features.\n\n\nCode\n# source: https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\n# Create the RFE object and compute a cross-validated score.\nsvc = RandomForestClassifier()\n\nmin_features_to_select = 1  # Minimum number of features to consider\nrfecv = RFECV(\n    estimator=svc,\n    step=1,\n    cv=StratifiedKFold(5),\n    scoring=\"roc_auc\",\n    min_features_to_select=min_features_to_select,\n)\nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (accuracy)\")\nplt.plot(\n    range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n    rfecv.grid_scores_,\n)\nplt.show()\n\n\nOptimal number of features : 2\n\n\nC:\\Users\\user\\anaconda3\\envs\\ml-challenge-env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:103: FutureWarning: The `grid_scores_` attribute is deprecated in version 1.0 in favor of `cv_results_` and will be removed in version 1.2.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n\n\n\nCode\nrfecv.get_feature_names_out()\n\n\narray(['B01', 'B09'], dtype=object)\n\n\n\n\nCode\nclf9 = make_pipeline_with_sampler(\n    RandomUnderSampler(random_state=0),\n    RandomForestClassifier(),\n)\nfeatures = ['B01', 'B09']\nclf9 = train_test_assess(data_no_clouds[features + ['label_id']], clf9)\ncalc_feats_importances(clf9[1], data_no_clouds[features])\n\n\nAccuracy: 97.59%\n\n\n{'B01': 50.180496359837235, 'B09': 49.819503640162765}"
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#satellite-images",
    "href": "posts/ML-Challenge/ML-Challenge.html#satellite-images",
    "title": "Crop classification in Satellite imagry",
    "section": "5. Satellite images",
    "text": "5. Satellite images\nNow that we have trained a model, let‚Äôs try to apply it to the images provided\n\n\nCode\ndef load_images(paths, mask_path=None):\n    \n    df = pd.DataFrame()\n    \n    for path in paths:\n        img = load_image(path)/10000\n        img = img.flatten()\n        df[path.split('.')[0].split('/')[1]] = img\n    \n    mask = load_image(mask_path)/255\n    mask = np.mean(mask, -1)\n    mask = mask.flatten() &gt; 0\n    df['mask'] = mask\n    \n    return df\n\n\n\n\nCode\nimgs = ['data/' + band +'.tif' for band in bands]\nsat_imgs = load_images(imgs, mask_path='data/mask.png')\n\n\n\n\nCode\ndef predict_label_image(model, images_df, bands, output_fname='predicted.png'):\n    # initialize pred_label column to 10 (a non valid label)\n    images_df.loc[:, 'pred_label'] = 10\n    # limit the prediction to the pixels where the mask is true\n    images_df.loc[images_df['mask'] == True, 'pred_label'] = model.predict(images_df.loc[images_df['mask'] == True, bands])\n    # create RGB predicted label image\n    images_df.loc[:,'R'] = 0\n    images_df.loc[:,'G'] = 0\n    images_df.loc[:,'B'] = 0\n\n    # when the predicted label is 0 (Other) set the value of pixel RED\n    images_df.loc[images_df['pred_label'] == 0, 'R'] = 255\n    # when the predicted label is 1 (Sorghum) set the value of pixel GREEN\n    images_df.loc[images_df['pred_label'] == 1, 'G'] = 255\n    # when the predicted label is 2 (Cotton) set the value of pixel BLUE\n    images_df.loc[images_df['pred_label'] == 2, 'B'] = 255\n\n    chans = ['R', 'G', 'B']\n    dims = (4000, 4000)\n    rgb_label = np.zeros(dims + (3, ), 'uint8')\n\n    for idx, chan in enumerate(chans):\n        img = images_df[chan].to_numpy()\n        rgb_label[..., idx] = img.reshape(dims)\n    \n    img = PIL.Image.fromarray(rgb_label)\n    img.save(output_fname)\n\n    return rgb_label\n\n\n\n\nCode\nfeatures = ['B01', 'B09']\nrgb_label = predict_label_image(clf9, sat_imgs.loc[:, features + ['mask']], features)\n\n\n\n\nCode\nfig = plt.figure(figsize = (20,20))\nax = fig.add_subplot(1,1,1) \nax.imshow(rgb_label )\n\n\n&lt;matplotlib.image.AxesImage at 0x1f009f5d7b0&gt;"
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#reminder",
    "href": "posts/ML-Challenge/ML-Challenge.html#reminder",
    "title": "Crop classification in Satellite imagry",
    "section": "Reminder",
    "text": "Reminder\n\nRed is Other\nGreen is Sorghum\nBlue is Cotton"
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#observations",
    "href": "posts/ML-Challenge/ML-Challenge.html#observations",
    "title": "Crop classification in Satellite imagry",
    "section": "Observations",
    "text": "Observations\n\nMany paddocks‚Äô predictions are noisy, especially for Sorghum and Other.\nThe boundary of the Cotton Paddocks are always mispredicted."
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#data-and-training-observation",
    "href": "posts/ML-Challenge/ML-Challenge.html#data-and-training-observation",
    "title": "Crop classification in Satellite imagry",
    "section": "Data and training observation",
    "text": "Data and training observation\n\nThe training data is highly imbalanced\nUsing weights for the classifier is not the best option\nThe generated features (NDVI and NDWI) are not discriminative"
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#next-steps",
    "href": "posts/ML-Challenge/ML-Challenge.html#next-steps",
    "title": "Crop classification in Satellite imagry",
    "section": "Next steps",
    "text": "Next steps\n\nIn the current experiment, we built a pixel classifier. We can build a patch level classifier as a next step, which can potentially reduce the class noise we are seeing in the earlier label image because it would provide more context to the classifier compared to the pixel level classifier.\nAnother potential solution to the class noise issue is to introduce more discriminative features by exploring various feature combinations as well as generating new features.\nImproving the quality of the training data by introducing a QA process to the data labelling. The QA process could involving relying on other data sources other than satellite images.\nHyperparameter tuning can be performed using cross validation and grid search methods as a next step to further improve the model performance (https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)."
  },
  {
    "objectID": "posts/ML-Challenge/ML-Challenge.html#resources",
    "href": "posts/ML-Challenge/ML-Challenge.html#resources",
    "title": "Crop classification in Satellite imagry",
    "section": "Resources",
    "text": "Resources\n\nImbalanced data\nSatellite image deep learning repo"
  },
  {
    "objectID": "posts/D3js-Explorations-3/index.html",
    "href": "posts/D3js-Explorations-3/index.html",
    "title": "D3 - Visualizing connections",
    "section": "",
    "text": "Today work is my own go at arc diagrams replicating a worked example at Matthew Clemens‚Äôs blog post. My code can be found at Github"
  }
]